{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, we originally ran this code in a separate file. We then transferred over the imporant code to this file. \n",
    "#We did not run it again because we did not want to get charged for API usage and were concerned we may go over the \n",
    "#limit of what's free if we doubled our total number of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import in the scraped data of schools we have\n",
    "df_MA = pd.read_csv('massachusetts.csv')\n",
    "df_VT = pd.read_csv('vermont.csv')\n",
    "df_CT = pd.read_csv('connecticut.csv')\n",
    "df_RI = pd.read_csv('rhode_island.csv')\n",
    "\n",
    "HS_MA = \"\"\n",
    "HS_VT = \"\"\n",
    "HS_CT = \"\"\n",
    "HS_RI = \"\"\n",
    "\n",
    "#Google Maps API allows \"Distance Matrices\", so long as there are pipes between each location\n",
    "for ad in df_MA[\"Address\"]:\n",
    "    HS_MA = HS_MA + ad+ \"|\"\n",
    "    \n",
    "for ad in df_VT[\"Address\"]:\n",
    "    HS_VT = HS_VT + ad+ \"|\"\n",
    "    \n",
    "for ad in df_CT[\"Address\"]:\n",
    "    HS_CT = HS_CT + ad+ \"|\"\n",
    "    \n",
    "for ad in df_RI[\"Address\"]:\n",
    "    HS_RI = HS_RI + ad+ \"|\"\n",
    "    \n",
    "#remove the final pipe character from each string\n",
    "HS_MA = HS_MA[0:-1]\n",
    "HS_VT = HS_VT[0:-1]\n",
    "HS_CT = HS_CT[0:-1]\n",
    "HS_RI = HS_RI[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "df_colleges = pd.DataFrame(columns = [\"College\", \"City\"])\n",
    "\n",
    "#retrieve a list of colleges from New England\n",
    "\n",
    "soup = BeautifulSoup(urllib.urlopen('https://nces.ed.gov/collegenavigator/?s=CT+ME+MA+NH+RI+VT&ic=1&pg=0').read())\n",
    "for link in soup.find_all(\"strong\"):\n",
    "    if link.text ==\"U.S. Department of Education\" or link.text ==\"13\" or link.text ==\"12\" or link.text == \"11\" or link.text ==\"10\" or link.text ==\"1\" or link.text ==\"2\" or link.text ==\"3\" or link.text ==\"4\" or link.text ==\"5\" or link.text ==\"6\" or link.text ==\"7\" or link.text ==\"8\" or link.text ==\"9\" or link.text == \"My Favorites\" or link.text == \"comparisons\":\n",
    "        pass\n",
    "    else:\n",
    "        tempdict= {\"College\": link.text, \"City\": link.parent.parent.text[len(link.text)::] }\n",
    "        df_colleges = df_colleges.append(tempdict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#break down the scraped list into unique towns, such that there's never more than one college per town\n",
    "\n",
    "df_colleges_unique = df_colleges.copy()\n",
    "seen_places = []\n",
    "del_indexes = []\n",
    "count = -1\n",
    "for row in df_colleges_unique[\"City\"]:\n",
    "    count = count + 1\n",
    "    if row not in seen_places:\n",
    "        seen_places.append(row)\n",
    "    else: \n",
    "        del_indexes.append(count)\n",
    "for index in del_indexes:\n",
    "    df_colleges_unique = df_colleges_unique.drop(index)\n",
    "df_colleges_unique=df_colleges_unique.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collegeaddstringMA = \"\"\n",
    "collegeaddstringVT = \"\"\n",
    "collegeaddstringRI = \"\"\n",
    "collegeaddstringCT = \"\"\n",
    "\n",
    "# creates four individual sets of the colleges, separated by state\n",
    "# Maine and New Hampshire not included because high schools did not have enough data\n",
    "\n",
    "for ind in range(0, len(df_colleges_unique[\"College\"])):\n",
    "    if \"Massachusetts\" in df_colleges_unique[\"City\"][ind]:\n",
    "        collegeaddstringMA = collegeaddstringMA + df_colleges_unique[\"College\"][ind]+ \"|\"\n",
    "    elif \"Vermont\" in df_colleges_unique[\"City\"][ind]:\n",
    "        collegeaddstringVT = collegeaddstringVT + df_colleges_unique[\"College\"][ind]+ \"|\"\n",
    "    elif \"Rhode Island\" in df_colleges_unique[\"City\"][ind]:\n",
    "        collegeaddstringRI = collegeaddstringRI + df_colleges_unique[\"College\"][ind]+ \"|\"\n",
    "    elif \"Connecticut\" in df_colleges_unique[\"City\"][ind]:\n",
    "        collegeaddstringCT = collegeaddstringCT + df_colleges_unique[\"College\"][ind]+ \"|\"\n",
    "    \n",
    "collegeaddstringMA = collegeaddstringMA[0:-1]\n",
    "collegeaddstringVT = collegeaddstringVT[0:-1]\n",
    "collegeaddstringRI = collegeaddstringRI[0:-1]\n",
    "collegeaddstringCT = collegeaddstringCT[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that takes a high school or college string for a state and splits into groups of ten\n",
    "# Google Maps API cannot return a matrix with more than 100 elements at once\n",
    "\n",
    "def splitby10(collegelist):\n",
    "    collgrid = []\n",
    "    for x in range((len(collegelist)//10)+1):\n",
    "        collstring = \"\"\n",
    "        i = x*10\n",
    "        while i <= min((x*10)+9, len(collegelist)-1):\n",
    "            collstring = collstring + str(collegelist[i])+\"|\"\n",
    "            i = i+1\n",
    "        collstring = collstring[0:-1]\n",
    "        collgrid.append(collstring)\n",
    "        \n",
    "    return collgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HS_MA_split=splitby10(HS_MA.split(\"|\"))\n",
    "HS_VT_split=splitby10(HS_VT.split(\"|\"))\n",
    "HS_CT_split=splitby10(HS_CT.split(\"|\"))\n",
    "HS_RI_split=splitby10(HS_RI.split(\"|\"))\n",
    "collegeMA_split = splitby10(collegeaddstringMA.split(\"|\"))\n",
    "collegeVT_split = splitby10(collegeaddstringVT.split(\"|\"))\n",
    "collegeCT_split = splitby10(collegeaddstringCT.split(\"|\"))\n",
    "collegeRI_split = [collegeaddstringRI] #less than 10 colleges in RI, but wanted to return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json \n",
    " \n",
    "api_key ='my_api_key'\n",
    "#we didn't want to put our API key on github- please let us know if you need to see it\n",
    "\n",
    "distance_dict = {}\n",
    "\n",
    "# the below was ran four times, each time with a different state\n",
    "# running it in a loop with all four states is possible, but takes an extremely long amount of time\n",
    "# it's also not efficient, given that we want to keep the CSVs separate until it's all over\n",
    "\n",
    "for x in range(len(HS_VT_split)): #runs the high schools 10 at a time\n",
    "    small_dict=None\n",
    "    source = HS_VT_split[x]\n",
    "    for i in range(len(collegeVT_split)): #for a set of 10 high schools, runs 10 colleges at a time\n",
    "        dest = collegeVT_split[i]\n",
    "        url ='https://maps.googleapis.com/maps/api/distancematrix/json?'\n",
    "        r = requests.get(url + 'origins=' + source +\n",
    "                   '&destinations=' + dest +\n",
    "                   '&key=' + api_key)\n",
    "        mat = r.json()\n",
    "        rows = mat[\"rows\"]\n",
    "        if small_dict: #keeps track of the distances for every high school\n",
    "            for f in range(len(HS_VT_split[x].split(\"|\"))):\n",
    "                small_dict[f][\"elements\"].extend(rows[f][\"elements\"])\n",
    "        else:\n",
    "            small_dict=rows\n",
    "                \n",
    "    l = []\n",
    "\n",
    "    for e in small_dict:\n",
    "        distance = 100000000 #arbitrarily large number that won't be surpassed\n",
    "        for d in e[\"elements\"]:\n",
    "            if d[\"distance\"][\"value\"] < distance:\n",
    "                distance = d[\"distance\"][\"value\"]\n",
    "        l.append(distance)\n",
    "    if distance_dict.get(\"Distance from Nearest College\"):\n",
    "        distance_dict[\"Distance from Nearest College\"].extend(l)\n",
    "    else:\n",
    "        distance_dict[\"Distance from Nearest College\"] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VT[\"Distance from Nearest College\"]=distance_dict[\"Distance from Nearest College\"] #add to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We noticed that a few of our addresses were duplicates. We removed the duplicate rows here\n",
    "import pandas as pd\n",
    "df_VT = df_VT.drop_duplicates(subset=\"Address\")\n",
    "df_VT = df_VT.reset_index(drop=True)\n",
    "df_VT = df_VT.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we sent the final results from each state to a csv (in GitHub and imported in our fp4 file)\n",
    "df_VT.to_csv('ver_with_dist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
